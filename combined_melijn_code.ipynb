{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce6bd18",
   "metadata": {},
   "source": [
    "N.B. Keep CSV updates separate to track errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a7d90",
   "metadata": {},
   "source": [
    "CSV 1: Extract the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries:\n",
    "    # 1. Requests - for HTTP requests to fetch data from a URL\n",
    "    # 2. ET - for parsing data from XML files\n",
    "    # 3. csv - for reading and writing \n",
    "    # 4. re - for pattern matching\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Base URL: needs to be replaced with ALTO-XML file link for each volume \n",
    "#           without the individual page - this will be iterated through later\n",
    "# Headers: accepts XML content (this was an addition from ChatGPT following errors)\n",
    "base_url = 'https://raw.githubusercontent.com/MoMu-Antwerp/melijn/main/altofiles/T94_192/alto/MOMU_T94_192_'\n",
    "headers = {'accept': 'application/xml;q=0.9, */*;q=0.8'}\n",
    "\n",
    "# Create a list to store the extracted transcription data:\n",
    "all_entries = []\n",
    "\n",
    "# Regular expression to pattern match the transcriptions for “Adij” (and HTR errors), the abbreviated form of \n",
    "# Anno Domini, which precedes entries and thus acts as a marker of new entries:\n",
    "# N.B. Regular expression must be adapted according to volume.\n",
    "regex_pattern = re.compile(r'^(adij|adj|adiy|ady|addyy|aeij|aey)', re.IGNORECASE)\n",
    "\n",
    "# For-loop to iterate through the range of files in each volume, before constructing URL for each file by \n",
    "# appending the base URL with the file number:\n",
    "for page_number in range(#set number of altofiles in folder):\n",
    "    file_number = f'{page_number:04d}'\n",
    "    url = f'{base_url}{file_number}.xml'\n",
    "\n",
    "    # Use the contructued URLs to fetch the XML content:\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # If-statement to ensure HTTP request was successful - this is important to monitor in automated iterations:\n",
    "    if response.status_code == 200:\n",
    "    \n",
    "        # Parse the XML content:\n",
    "        root = ET.fromstring(response.text)\n",
    "    \n",
    "        # Create variables to store the URL of the current file and the transcription details:\n",
    "        current_filename = url\n",
    "        current_entry_content = []\n",
    "        first_hpos, first_vpos, first_width, first_height = None, None, None, None\n",
    "        last_hpos, last_vpos, last_width, last_height = None, None, None, None\n",
    "        first_textline_id = None\n",
    "\n",
    "        # For-loop to iterate through each TextLine in the XML content and extract the content for above variables:\n",
    "        for text_line in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}TextLine'):\n",
    "            content = text_line.find('.//{http://www.loc.gov/standards/alto/ns-v4#}String').get('CONTENT', '')\n",
    "            textline_id = text_line.get('ID', '')\n",
    "            hpos = int(text_line.get('HPOS', 0))\n",
    "            vpos = int(text_line.get('VPOS', 0))\n",
    "            width = int(text_line.get('WIDTH', 0))\n",
    "            height = int(text_line.get('HEIGHT', 0))\n",
    "            \n",
    "            # Skip empty lines:\n",
    "            if content.strip() == '':\n",
    "                continue\n",
    "\n",
    "            # If the content matches the regular expression pattern indicating a new entry, \n",
    "            # start a new entry and append the created list:\n",
    "            if regex_pattern.match(content):\n",
    "                if current_entry_content:\n",
    "                    all_entries.append({\n",
    "                        'filename': current_filename,\n",
    "                        'content': '\\n'.join(current_entry_content),\n",
    "                        'textline_id': first_textline_id,\n",
    "                        'first_hpos': first_hpos,\n",
    "                        'first_vpos': first_vpos,\n",
    "                        'first_width': first_width,\n",
    "                        'first_height': first_height,\n",
    "                        'last_hpos': last_hpos,\n",
    "                        'last_vpos': last_vpos,\n",
    "                        'last_width': last_width,\n",
    "                        'last_height': last_height\n",
    "                    })\n",
    "                    current_entry_content = []  \n",
    "\n",
    "                first_hpos, first_vpos, first_width, first_height = hpos, vpos, width, height\n",
    "                last_hpos, last_vpos, last_width, last_height = hpos, vpos, width, height\n",
    "                first_textline_id = textline_id\n",
    "            else:\n",
    "                last_hpos, last_vpos, last_width, last_height = hpos, vpos, width, height\n",
    "\n",
    "            current_entry_content.append(content)\n",
    "\n",
    "        # Store the information from the current entry in the all_entries list:\n",
    "        if current_entry_content:\n",
    "            all_entries.append({\n",
    "                'filename': current_filename,\n",
    "                'content': '\\n'.join(current_entry_content),\n",
    "                'textline_id': first_textline_id,\n",
    "                'first_hpos': first_hpos,\n",
    "                'first_vpos': first_vpos,\n",
    "                'first_width': first_width,\n",
    "                'first_height': first_height,\n",
    "                'last_hpos': last_hpos,\n",
    "                'last_vpos': last_vpos,\n",
    "                'last_width': last_width,\n",
    "                'last_height': last_height\n",
    "            })\n",
    "\n",
    "    # If HTTP request was unsuccessful, return the page causing error to allow human inspection:\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the XML content for Page {page_number}. Status code: {response.status_code}\")\n",
    "\n",
    "# Create a new CSV file and add all collected entries to it, with columns for each type of data:\n",
    "csv_filename = 'MoMu_T94192v2.csv' #replace with name of volume\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['filename', 'content', 'textline_id', 'first_hpos', 'first_vpos', 'first_width', 'first_height',\n",
    "                  'last_hpos', 'last_vpos', 'last_width', 'last_height']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for entry in all_entries:\n",
    "        writer.writerow(entry)\n",
    "\n",
    "print(f\"CSV file '{csv_filename}' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ef1fe",
   "metadata": {},
   "source": [
    "CSV 2: Identifying languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library (gcld3) for guessing languages:\n",
    "!pip install gcld3\n",
    "import gcld3\n",
    "\n",
    "# Define a function to take text and detect its language (written with support from ChatGPT):\n",
    "def detect_language_gcld3(text):\n",
    "    try:\n",
    "        detector = gcld3.NNetLanguageIdentifier(min_num_bytes=0, max_num_bytes=1000)\n",
    "        result = detector.FindLanguage(text=text)\n",
    "        lang_detected = result.language\n",
    "        return lang_detected\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "\n",
    "# Load the CSV file into a DataFrame:\n",
    "df = pd.read_csv(#path to CSV1)\n",
    "\n",
    "# Create a language column by applying the function to the transcription:\n",
    "df['language'] = df['content'].apply(detect_language_gcld3)\n",
    "\n",
    "#Write the changes using a new CSV file name:\n",
    "df.to_csv(#updated_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33759b5a",
   "metadata": {},
   "source": [
    "CSV 3: Adding persistent identifiers (PID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16080a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths:\n",
    "melijn_csv = #path to file above\n",
    "updated_csv = #new csv name\n",
    "id_txt = #MelijnNOIDs.txt (text file of generated PIDs)\n",
    "updated_ids = #updatedNOID txt filename\n",
    "\n",
    "# Read the latest version of the CSV file and add each row to a new list:\n",
    "existing_csv = []\n",
    "with open(melijn_csv, 'r', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        existing_csv.append(row)\n",
    "\n",
    "# Read the text file of PIDs and add the four-character ID to a list\n",
    "with open(id_txt, 'r') as f:\n",
    "    ids = [line.strip()[4:] for line in f.readlines()]\n",
    "\n",
    "# Create a new CSV file to add the PID values:\n",
    "with open(updated_csv, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    # For-loop to iterate through each row in the CSV file:\n",
    "    for i, row in enumerate(existing_csv):\n",
    "        \n",
    "        # If-statement to ensure there are enough IDs for each row\n",
    "        if i < len(ids):\n",
    "            \n",
    "            # Add the new PID to the existing MoMu system and update the CSV file:\n",
    "            new_id = \"http://data.momu.be/ark:34546/m63xtm_\" + ids[i]\n",
    "            row.append(new_id)\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Calculate the number of remaining unused IDs depending on the last used ID\n",
    "# and write a new tect file saving the remaining IDs:\n",
    "unused_ids = ids[len(existing_csv):]\n",
    "with open(updated_ids, 'w') as f:\n",
    "    for id_ in unused_ids:\n",
    "        f.write(\"id: \" + id_ + \"\\n\")\n",
    "        \n",
    "\n",
    "# Update the CSV file with PIDs to rename the column for PIDs\n",
    "df = pd.read_csv(updated_csv)\n",
    "df.rename(columns={\"http://data.momu.be/ark:34546/m63xtm_XXXXX\": \"pid\"}, inplace=True)\n",
    "#XXXXX needs to be replaced by first PID in textfile\n",
    "df.to_csv(updated_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e77a1",
   "metadata": {},
   "source": [
    "Then used OpenRefine to do the following:\n",
    "<li>Add resourceTemplate column with \"Melijn entry\" as content for all rows</li>\n",
    "<li>Add title column with \"Letter from Melijn archive\" as content for all rows</li>\n",
    "<li>Add dateCreated column using transcription date in volume manifest</li>\n",
    "<li>Add description column based on altofile (filename) column using value.replace(\"raw_github_link up to alto/\", \"\").replace(\".xml\", \"\")</li>\n",
    "<li>Add partOf column using the Omeka ID for the volume as content for all rows</li>\n",
    "<li>Add pageStart column based on altofile (filename) column using value.slive(value.lastIndexOf('_')+1, value.lastIndexOf('.'))</li>\n",
    "<li>Add reviewStatus column with \"unreviewed\" as content for all rows</li>\n",
    "<li>Use text facet to check irregularities in language column</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a891e",
   "metadata": {},
   "source": [
    "CSV 4: Adding Universal Viewer link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the updated CSV from the OpenRefine edits:\n",
    "df = pd.read_csv(#filepath)\n",
    "\n",
    "# Define a function to extract the canvas view value depending on the page number:\n",
    "def extract_cv(pages):\n",
    "    \n",
    "    # Convert the page number into an integer and extract the last four digits:\n",
    "    cv_value = int(pages) % 10000\n",
    "    \n",
    "    # If-statement to avoid invalid numbers, otherwise minus one, as the number in the link is one less \n",
    "    # than the file number:\n",
    "    if cv_value == 0:\n",
    "        cv_value = 9999\n",
    "    elif cv_value > 0:\n",
    "        cv_value = cv_value-1 \n",
    "    return cv_value\n",
    "\n",
    "# Apply the function to the page number column:\n",
    "df['cv'] = df['pageStart'].apply(extract_cv)\n",
    "\n",
    "# Define a function to construct a link to the universal viewer using each page's CV number:\n",
    "def construct_link(cv):\n",
    "    return f\"https://universalviewer.io/uv.html?manifest=https://museumstichting.resourcespace.com/iiif/11524/manifest#?c=0&m=0&s=0&cv={cv}&xywh=-1391%2C0%2C10494%2C4670\"\n",
    "    # Link needs to be replaced by the correct universal viewer link for that volume. \n",
    "    # Ensure that \"cv={cv}\" is intact\n",
    "\n",
    "# Apply the function to each CV value in the DataFrame and add the link under a new column:\n",
    "df['edm:isShownBy'] = df['cv'].apply(construct_link)\n",
    "\n",
    "df.to_csv(, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce6da8",
   "metadata": {},
   "source": [
    "CSV 5: Adding IIIF manifest information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37792392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the updated CSV 4:\n",
    "df = pd.read_csv(#filepath)\n",
    "\n",
    "manifest_url = #raw json manifest link from the file\n",
    "\n",
    "# Fetch the manifest from the URL link:\n",
    "response = requests.get(manifest_url)\n",
    "if response.status_code == 200:\n",
    "    manifest = response.json()\n",
    "else:\n",
    "    print(\"Failed to retrieve the IIIF manifest.\")\n",
    "    exit()\n",
    "\n",
    "# Define a function to extract the IIIF ID for each canvas (page) number by retrieving the canvas number \n",
    "# and constructing the IIIF ID:\n",
    "def extract_iiif_id(canvas_number):\n",
    "    canvas = manifest['sequences'][0]['canvases'][canvas_number]\n",
    "    thumbnail_service_id = canvas['thumbnail']['service']['@id']\n",
    "    iiif_id = thumbnail_service_id + \"/info.json\"\n",
    "    return iiif_id\n",
    "\n",
    "# Create a list to store IIIF ID:\n",
    "iiif_ids = []\n",
    "\n",
    "# For-loop to iterate through each row to get the canvas number:\n",
    "for index, row in df.iterrows():\n",
    "    page_number = row['pagestart'] - 1  \n",
    "    \n",
    "    # Apply the function to get the IIIF ID and append the IIIF ID list:\n",
    "    iiif_id = extract_iiif_id(page_number)\n",
    "    iiif_ids.append(iiif_id)\n",
    "\n",
    "# Add a new column to the DataFrame and create a new CSV file:\n",
    "df['IIIF_id'] = iiif_ids\n",
    "output_csv_filename = #new filename\n",
    "df.to_csv(output_csv_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
